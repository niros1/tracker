{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install supervision\n",
    "!ln -s /home/ubuntu/gdino/GroundingDINO/groundingdino .\n",
    "!ln -s /home/ubuntu/gdino/GroundingDINO/weights ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"weights/groundingdino_swint_ogc.pth\")\n",
    "GDINO_PATH = \"/home/ubuntu/gdino/GroundingDINO\"\n",
    "IMAGE_PATH = f\"{GDINO_PATH}/.asset/cat_dog.jpeg\"\n",
    "TEXT_PROMPT = \"chair . person . dog .\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Tuple\n",
    "import cv2\n",
    "from scipy.fftpack import sc_diff\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from typing import Any, Generator\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import groundingdino.datasets.transforms as T\n",
    "\n",
    "def is_iterable(obj):\n",
    "    try:\n",
    "        iter(obj)\n",
    "        return True\n",
    "    except TypeError:\n",
    "        return False\n",
    "class Stack:\n",
    "    def __init__(self, max_size):\n",
    "        self.stack = []\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def push(self, item):\n",
    "        if len(self.stack) == self.max_size:\n",
    "            self.stack.pop(0)  # Remove the oldest item\n",
    "        self.stack.append(item)\n",
    "\n",
    "    def pop(self):\n",
    "        if len(self.stack) < 1:\n",
    "            return None\n",
    "        return self.stack.pop()\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.stack)\n",
    "    def __str__(self):\n",
    "        return str(self.stack)\n",
    "    def __iter__(self):\n",
    "        return iter(self.stack)\n",
    "\n",
    "def extract_frames(video_path, output_folder, frames_limit=100, skip=0):\n",
    "    \"\"\"\n",
    "        write each frame to a file\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if video opened successfully\n",
    "    if not video.isOpened():\n",
    "        print(\"Could not open video\")\n",
    "        return\n",
    "\n",
    "    frame_taken = 0\n",
    "    iteration = -1\n",
    "    success = True\n",
    "    files = []\n",
    "    while (frames_limit > 0 and frame_taken < frames_limit) or (frames_limit == 0 and success is True):\n",
    "        iteration += 1\n",
    "        \n",
    "        # Read the next frame from the video. If you read at the end of the video, success will be False\n",
    "        success, frame = video.read()\n",
    "        # print(frame_count)\n",
    "\n",
    "        # Break the loop if the video is finished\n",
    "        if not success:\n",
    "            break\n",
    "        if skip != 0 and iteration % skip != 0:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "\n",
    "        # Save the frame into the output folder\n",
    "        cv2.imwrite(f\"{output_folder}/frame{frame_taken}.jpg\", frame)\n",
    "        files.append(f\"{output_folder}/frame{frame_taken}.jpg\")\n",
    "\n",
    "        frame_taken +=1\n",
    "\n",
    "    # Release the video file\n",
    "    video.release()\n",
    "    return files\n",
    "\n",
    "def generate_frames(video_file: str, frames_limit=10) -> Generator[np.ndarray, None, None]:\n",
    "    \"\"\"\n",
    "        yield each frame as byte array\n",
    "    \"\"\"\n",
    "    video = cv2.VideoCapture(video_file)\n",
    "    frame_count = 0\n",
    "\n",
    "    while video.isOpened():\n",
    "        success, frame = video.read()\n",
    "\n",
    "        if not ((frames_limit > 0 and frame_count < frames_limit) or (frames_limit == 0 and success is True)):\n",
    "            break\n",
    "\n",
    "        yield frame\n",
    "        frame_count += 1\n",
    "\n",
    "    video.release()\n",
    "\n",
    "def plot_image(image: np.ndarray, size: int = 12) -> None:\n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(image[...,::-1])\n",
    "    plt.show()\n",
    "\n",
    "def zoom_at(img, zoom=1, angle=0, coord=None):\n",
    "    \n",
    "    cy, cx = [ i/2 for i in img.shape[:-1] ] if coord is None else coord[::-1]\n",
    "    \n",
    "    rot_mat = cv2.getRotationMatrix2D((cx,cy), angle, zoom)\n",
    "    result = cv2.warpAffine(img, rot_mat, img.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def convert_ndarray(frame: np.ndarray[Any]) ->  torch.Tensor:\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_source = Image.fromarray(frame_rgb)\n",
    "\n",
    "    # image_source = Image.fromarray(arr).convert(\"RGB\")\n",
    "    # image_source = Image.open(image_path).convert(\"RGB\")\n",
    "    # image_source = Image.open(image_path).convert(\"RGB\")\n",
    "    # image = np.asarray(image_source)\n",
    "    image_transformed, _ = transform(image_source, None)\n",
    "    return image_transformed\n",
    "\n",
    "def add_text_to_frame2(frame, text, position=(50, 50), font_scale=1, font_color=(0, 0, 255), thickness=4):\n",
    "    \"\"\"\n",
    "    Adds text to a single frame.\n",
    "    \"\"\"\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    if is_iterable(text):\n",
    "        for line in text:\n",
    "            cv2.putText(frame, line, position, font, font_scale, font_color, thickness)\n",
    "            # position = (position[0], position[1] + 50)\n",
    "        # text = \" \".join(text)\n",
    "    else:\n",
    "        cv2.putText(frame, text, position, font, font_scale, font_color, thickness)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/mid/frame0.jpg',\n",
       " 'output/mid/frame1.jpg',\n",
       " 'output/mid/frame2.jpg',\n",
       " 'output/mid/frame3.jpg',\n",
       " 'output/mid/frame4.jpg',\n",
       " 'output/mid/frame5.jpg',\n",
       " 'output/mid/frame6.jpg',\n",
       " 'output/mid/frame7.jpg',\n",
       " 'output/mid/frame8.jpg',\n",
       " 'output/mid/frame9.jpg',\n",
       " 'output/mid/frame10.jpg',\n",
       " 'output/mid/frame11.jpg',\n",
       " 'output/mid/frame12.jpg',\n",
       " 'output/mid/frame13.jpg',\n",
       " 'output/mid/frame14.jpg',\n",
       " 'output/mid/frame15.jpg',\n",
       " 'output/mid/frame16.jpg',\n",
       " 'output/mid/frame17.jpg',\n",
       " 'output/mid/frame18.jpg',\n",
       " 'output/mid/frame19.jpg',\n",
       " 'output/mid/frame20.jpg',\n",
       " 'output/mid/frame21.jpg',\n",
       " 'output/mid/frame22.jpg',\n",
       " 'output/mid/frame23.jpg',\n",
       " 'output/mid/frame24.jpg',\n",
       " 'output/mid/frame25.jpg',\n",
       " 'output/mid/frame26.jpg',\n",
       " 'output/mid/frame27.jpg']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = extract_frames(\n",
    "    video_path=\"input/basketball.mp4\", \n",
    "    output_folder=\"output/mid\", \n",
    "    frames_limit=100, \n",
    "    skip=100\n",
    ")\n",
    "files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1520, 2704), 59.94005994005994)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "vid_cap = cv2.VideoCapture('input/basketball.mp4')\n",
    "fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "ret, frame = vid_cap.read()\n",
    "vid_cap.release()\n",
    "resolution_size = (int(frame.shape[0]), int(frame.shape[1]))\n",
    "resolution_size, fps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# size = (int(img0.shape[1]/2), int(img0.shape[0]/2))\n",
    "resolution = (int(1024), int(1024))\n",
    "# Create a new video\n",
    "h264 = cv.VideoWriter_fourcc('h','2','6','4')\n",
    "mp4v = cv.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "new_video = cv2.VideoWriter(\"new_video1.mp4\", mp4v, fps, resolution)\n",
    "\n",
    "for file in files:\n",
    "    f_name = os.path.basename(file)\n",
    "    print(f_name)\n",
    "    image_source, image = load_image(file)\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=image,\n",
    "        caption=\"basketball\",\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "    if boxes.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    x = xyxy[0][0]\n",
    "    y = xyxy[0][1]\n",
    "\n",
    "    # cv2.imwrite(f\"output/predict/{f_name}\", annotated_frame)\n",
    "    # cv.imwrite('zoom_frame0.jpg', zoom_at(annotated_frame, 1.5, coord=(264.5, 275)) )\n",
    "    \n",
    "    # plot_image(annotated_frame, 8)\n",
    "    # plot_image(zoom_at(annotated_frame, 2, coord=(x, y)), 1280)\n",
    "    new_video.write(annotated_frame)\n",
    "    # print(boxes, logits, phrases)\n",
    "    print(\"--------------\")\n",
    "\n",
    "print(\"Releasing video\")\n",
    "new_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y -c conda-forge ipywidgets # or pip install ipywidgets\n",
    "# !conda install -n base -c conda-forge jupyterlab_widgets\n",
    "\n",
    "# !conda install -n base -c conda-forge widgetsnbextension\n",
    "# !which python\n",
    "# !pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import groundingdino.datasets.transforms as T\n",
    "import PIL\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "def load_ndarr_image(numpy_image) -> Tuple[np.array, torch.Tensor]:\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    # image_source = Image.open(image_path).convert(\"RGB\")\n",
    "    PIL_image = Image.fromarray(np.uint8(numpy_image)).convert('RGB')\n",
    "\n",
    "    image = np.asarray(PIL_image)\n",
    "    image_transformed, _ = transform(PIL_image, None)\n",
    "    return image, image_transformed\n",
    "\n",
    "frame_iterator = iter(generate_frames(video_file=\"input/basketball.mp4\", frames_limit=5))\n",
    "frames_data = []\n",
    "for frame in tqdm(frame_iterator, total=5):\n",
    "    # print(frame)\n",
    "    print(\"-------------------\")\n",
    "    image_source, image = load_ndarr_image(frame)\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=image,\n",
    "        caption=\"basketball\",\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "    frames_data.append((boxes, logits, phrases))\n",
    "\n",
    "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "\n",
    "    # cv2.imwrite(f\"output/predict/{f_name}\", annotated_frame)\n",
    "    print(boxes, logits, phrases, xyxy)\n",
    "    plot_image(annotated_frame, 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('output/mid/frame0.jpg')\n",
    "z_img = zoom_at(img, 1.5, coord=(264.5, 275))\n",
    "# print(z_img)\n",
    "    \n",
    "\n",
    "cv.imwrite('frame0.jpg', img )\n",
    "cv.imwrite('zoom_frame0.jpg', zoom_at(img, 1.5, coord=(264.5, 275)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install -y ffmpeg x264 libx264-dev\n",
    "# !pip install jupyterlab_widgets ipywidgets\n",
    "!pip install --upgrade ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Stack()\n",
    "x.push(\"Hello\")\n",
    "x.push(\"World\")\n",
    "\n",
    "\n",
    "def write_frame(vid_writer, source_frame, history: Stack,  boxes, logits, phrases):\n",
    "    annotated_frame = annotate(image_source=source_frame, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    h, w, _ = frame.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    x = xyxy[0][0]\n",
    "    y = xyxy[0][1]\n",
    "    zoom_frame = zoom_at(frame, 2, coord=(x, y))\n",
    "\n",
    "    print(\"Zoom ->>>>>\", (x, y))\n",
    "    history.push(f\"Zoom at: {x}, {y} ---- phrases: {phrases}\")\n",
    "    add_text_to_frame2(zoom_frame, history, position=(50, 150))\n",
    "    vid_writer.write(zoom_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "----------- 1668703592.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf88ce73e2c419688e82b5858ec6ac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoom ->>>>> (416.02338, 858.37134)\n",
      "Count: 1Zoom ->>>>> (416.93814, 862.1715)\n",
      "Count: 2Zoom ->>>>> (413.7442, 864.8559)\n",
      "Count: 3Zoom ->>>>> (418.1336, 869.54034)\n",
      "Count: 4Zoom ->>>>> (413.4619, 873.45197)\n",
      "Count: 5Zoom ->>>>> (409.07446, 877.97296)\n",
      "Count: 6Zoom ->>>>> (409.99716, 883.97845)\n",
      "Count: 7Zoom ->>>>> (410.34915, 884.84686)\n",
      "Count: 8Zoom ->>>>> (407.07065, 890.44293)\n",
      "Count: 9Zoom ->>>>> (407.07065, 890.44293)\n",
      "Count: 9Zoom ->>>>> (407.4342, 896.73676)\n",
      "Count: 10Zoom ->>>>> (407.57465, 897.67487)\n",
      "Count: 11Zoom ->>>>> (611.7635, 801.2432)\n",
      "Count: 12Zoom ->>>>> (408.3813, 899.4302)\n",
      "Count: 13Zoom ->>>>> (409.71753, 899.5812)\n",
      "Count: 14Zoom ->>>>> (410.57266, 899.47534)\n",
      "Count: 15Zoom ->>>>> (411.42215, 899.6825)\n",
      "Count: 16Zoom ->>>>> (414.72546, 899.962)\n",
      "Count: 17Zoom ->>>>> (416.83835, 900.0826)\n",
      "Count: 18Zoom ->>>>> (418.22287, 900.07965)\n",
      "Count: 19Zoom ->>>>> (417.86465, 899.1971)\n",
      "Count: 20Zoom ->>>>> (418.0332, 898.4385)\n",
      "Count: 21Zoom ->>>>> (418.9025, 897.1738)\n",
      "Count: 22Zoom ->>>>> (418.21854, 896.08655)\n",
      "Count: 23Zoom ->>>>> (418.55984, 895.4949)\n",
      "Count: 24Zoom ->>>>> (419.1223, 895.5639)\n",
      "Count: 25Zoom ->>>>> (419.05408, 895.5997)\n",
      "Count: 26Zoom ->>>>> (419.22458, 894.99036)\n",
      "Count: 27Zoom ->>>>> (419.80936, 894.0711)\n",
      "Count: 28Zoom ->>>>> (420.1826, 893.6385)\n",
      "Count: 29Zoom ->>>>> (419.8429, 893.4634)\n",
      "Count: 30Zoom ->>>>> (420.4158, 893.74066)\n",
      "Count: 31Zoom ->>>>> (420.25916, 894.9506)\n",
      "Count: 32Zoom ->>>>> (420.90375, 895.6376)\n",
      "Count: 33Zoom ->>>>> (419.42493, 895.8361)\n",
      "Count: 34Zoom ->>>>> (416.7999, 896.9468)\n",
      "Count: 35Zoom ->>>>> (414.01212, 898.67596)\n",
      "Count: 36Zoom ->>>>> (412.28656, 899.94183)\n",
      "Count: 37Zoom ->>>>> (411.74738, 901.989)\n",
      "Count: 38Zoom ->>>>> (411.74738, 901.989)\n",
      "Count: 38Zoom ->>>>> (411.74738, 901.989)\n",
      "Count: 38Zoom ->>>>> (411.74738, 901.989)\n",
      "Count: 38Zoom ->>>>> (411.74738, 901.989)\n",
      "Count: 38Zoom ->>>>> (411.74738, 901.989)\n",
      "Count: 38Zoom ->>>>> (411.74738, 901.989)\n",
      "Count: 38Zoom ->>>>> (415.8726, 900.8058)\n",
      "Count: 39Zoom ->>>>> (418.3231, 900.8246)\n",
      "Count: 40Zoom ->>>>> (422.40375, 900.363)\n",
      "Count: 41Zoom ->>>>> (428.84018, 900.95905)\n",
      "Count: 42Zoom ->>>>> (433.6381, 900.5207)\n",
      "Count: 43Zoom ->>>>> (438.2668, 900.80133)\n",
      "Count: 44Zoom ->>>>> (438.2668, 900.80133)\n",
      "Count: 44Zoom ->>>>> (438.2668, 900.80133)\n",
      "Count: 44Zoom ->>>>> (438.2668, 900.80133)\n",
      "Count: 44Zoom ->>>>> (438.2668, 900.80133)\n",
      "Count: 44Zoom ->>>>> (426.09485, 860.3171)\n",
      "Count: 45Zoom ->>>>> (426.09485, 860.3171)\n",
      "Count: 45Zoom ->>>>> (426.09485, 860.3171)\n",
      "Count: 45Zoom ->>>>> (426.09485, 860.3171)\n",
      "Count: 45Zoom ->>>>> (426.09485, 860.3171)\n",
      "Count: 45Zoom ->>>>> (426.09485, 860.3171)\n",
      "Count: 45Zoom ->>>>> (426.09485, 860.3171)\n",
      "Count: 45Zoom ->>>>> (426.09485, 860.3171)\n",
      "Count: 45Zoom ->>>>> (596.5044, 788.147)\n",
      "Count: 46Zoom ->>>>> (596.4878, 788.1933)\n",
      "Count: 47Zoom ->>>>> (596.8065, 788.3755)\n",
      "Count: 48Zoom ->>>>> (597.6413, 787.9826)\n",
      "Count: 49Zoom ->>>>> (597.99457, 788.14795)\n",
      "Count: 50Zoom ->>>>> (597.1442, 788.9042)\n",
      "Count: 51Zoom ->>>>> (597.1356, 788.92206)\n",
      "Count: 52Zoom ->>>>> (597.61145, 788.80817)\n",
      "Count: 53Zoom ->>>>> (598.85034, 789.1004)\n",
      "Count: 54Zoom ->>>>> (543.66327, 913.1617)\n",
      "Count: 55Zoom ->>>>> (546.1326, 911.03754)\n",
      "Count: 56Zoom ->>>>> (544.2807, 908.17194)\n",
      "Count: 57Zoom ->>>>> (546.9084, 906.33)\n",
      "Count: 58Zoom ->>>>> (607.52185, 791.64264)\n",
      "Count: 59Zoom ->>>>> (547.70123, 901.38794)\n",
      "Count: 60Zoom ->>>>> (546.4559, 895.4535)\n",
      "Count: 61Zoom ->>>>> (546.4559, 895.4535)\n",
      "Count: 61Zoom ->>>>> (612.1365, 793.2518)\n",
      "Count: 62Zoom ->>>>> (612.9969, 793.41846)\n",
      "Count: 63Zoom ->>>>> (612.9969, 793.41846)\n",
      "Count: 63Zoom ->>>>> (612.9969, 793.41846)\n",
      "Count: 63Zoom ->>>>> (612.9969, 793.41846)\n",
      "Count: 63Zoom ->>>>> (561.8262, 883.6593)\n",
      "Count: 64Zoom ->>>>> (561.6207, 882.8859)\n",
      "Count: 65Zoom ->>>>> (565.2407, 882.5304)\n",
      "Count: 66Zoom ->>>>> (568.9856, 880.98895)\n",
      "Count: 67Zoom ->>>>> (576.15283, 880.40546)\n",
      "Count: 68Zoom ->>>>> (580.61816, 879.23724)\n",
      "Count: 69Zoom ->>>>> (584.47736, 877.5712)\n",
      "Count: 70Zoom ->>>>> (590.11633, 875.70874)\n",
      "Count: 71Zoom ->>>>> (595.15594, 873.90393)\n",
      "Count: 72Zoom ->>>>> (599.1477, 872.5662)\n",
      "Count: 73Zoom ->>>>> (599.1477, 872.5662)\n",
      "Count: 73Zoom ->>>>> (660.91895, 815.41956)\n",
      "Count: 74Zoom ->>>>> (660.62244, 815.7254)\n",
      "Count: 75Zoom ->>>>> (660.62244, 815.7254)\n",
      "Count: 75Zoom ->>>>> (660.62244, 815.7254)\n",
      "Count: 75Zoom ->>>>> (660.62244, 815.7254)\n",
      "Count: 75Zoom ->>>>> (516.2651, 859.3252)\n",
      "Count: 76Zoom ->>>>> (643.31775, 856.9487)\n",
      "Count: 77Zoom ->>>>> (643.31775, 856.9487)\n",
      "Count: 77Zoom ->>>>> (643.31775, 856.9487)\n",
      "Count: 77Zoom ->>>>> (651.307, 847.6281)\n",
      "Count: 78Zoom ->>>>> (654.21356, 844.98553)\n",
      "Count: 79Zoom ->>>>> (657.0699, 842.8632)\n",
      "Count: 80Zoom ->>>>> (658.3753, 838.25336)\n",
      "Count: 81Zoom ->>>>> (657.24286, 834.34906)\n",
      "Count: 82Zoom ->>>>> (657.51996, 832.5307)\n",
      "Count: 83Zoom ->>>>> (659.2419, 830.8456)\n",
      "Count: 84Zoom ->>>>> (659.2419, 830.8456)\n",
      "Count: 84Zoom ->>>>> (659.2419, 830.8456)\n",
      "Count: 84Zoom ->>>>> (659.2419, 830.8456)\n",
      "Count: 84Zoom ->>>>> (659.2419, 830.8456)\n",
      "Count: 84Zoom ->>>>> (659.2419, 830.8456)\n",
      "Count: 84Zoom ->>>>> (668.40857, 803.9017)\n",
      "Count: 85Zoom ->>>>> (685.3439, 799.4965)\n",
      "Count: 86Zoom ->>>>> (699.31354, 794.15814)\n",
      "Releasing video\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "h264 = cv2.VideoWriter_fourcc('h','2','6','4')\n",
    "mp4v = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "mp4v_2 = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "vid_name = \"output/new_video1.mp4\"\n",
    "\n",
    "\n",
    "# vid_cap = cv.VideoCapture(\"input/basketball.mp4\")\n",
    "\n",
    "# fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "# ret, frame = vid_cap.read()\n",
    "# vid_cap.release()\n",
    "# resolution_size = (int(frame.shape[0]), int(frame.shape[1]))\n",
    "\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"weights/groundingdino_swint_ogc.pth\")\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "cap = cv2.VideoCapture(\"input/basketball.mp4\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# cap.open(\"input/basketball.mp4\")\n",
    "exist_fourcc = cap.get(cv2.CAP_PROP_FOURCC)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "\n",
    "cap.release()\n",
    "print(\"-----------\", exist_fourcc)\n",
    "# return\n",
    "try:\n",
    "    os.remove(vid_name)\n",
    "except:\n",
    "    pass\n",
    "new_video = cv2.VideoWriter(vid_name, fourcc, fps, (width, height))\n",
    "\n",
    "\n",
    "if not new_video.isOpened():\n",
    "    print(\"Error: Could not open output video.\")\n",
    "    exit()\n",
    "\n",
    "test\n",
    "frame_iterator = iter(generate_frames(video_file=\"input/basketball.mp4\", frames_limit=120))\n",
    "frames_data = []\n",
    "counter = 0\n",
    "previouse_state = {}\n",
    "history = Stack(15)\n",
    "for frame in tqdm(frame_iterator, total=5):\n",
    "    # print(frame)\n",
    "    print(f'\\rCount: {counter}', end='', flush=True)\n",
    "    \n",
    "    transformed_array = convert_ndarray(frame)\n",
    "\n",
    "\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=transformed_array,\n",
    "        caption=\"basketball\",\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "    # print(boxes, logits, phrases)\n",
    "\n",
    "    # Can't detect any object\n",
    "    if boxes.shape[0] == 0:\n",
    "        write_frame(new_video, frame, history, previouse_state[\"boxes\"], previouse_state[\"logits\"], previouse_state[\"phrases\"])\n",
    "        continue\n",
    "\n",
    "    previouse_state = {\n",
    "        \"boxes\": boxes,\n",
    "        \"logits\": logits,\n",
    "        \"phrases\": phrases\n",
    "    }\n",
    "    write_frame(new_video, frame, history,  boxes, logits, phrases)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Releasing video\")\n",
    "new_video.release()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
