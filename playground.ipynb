{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: supervision in /home/ubuntu/.venv/lib/python3.10/site-packages (0.18.0)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from supervision) (0.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from supervision) (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /home/ubuntu/.venv/lib/python3.10/site-packages (from supervision) (1.26.4)\n",
      "Requirement already satisfied: opencv-python-headless>=4.5.5.64 in /home/ubuntu/.venv/lib/python3.10/site-packages (from supervision) (4.9.0.80)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/ubuntu/.venv/lib/python3.10/site-packages (from supervision) (6.0.1)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from supervision) (1.12.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ubuntu/.venv/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.venv/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/ubuntu/.venv/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/ubuntu/.venv/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.venv/lib/python3.10/site-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.16.0)\n",
      "ln: failed to create symbolic link './groundingdino': File exists\n"
     ]
    }
   ],
   "source": [
    "!pip install supervision\n",
    "!ln -s /home/ubuntu/gdino/GroundingDINO/groundingdino .\n",
    "!ln -s /home/ubuntu/gdino/GroundingDINO/weights ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:962: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import cv2\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"weights/groundingdino_swint_ogc.pth\")\n",
    "GDINO_PATH = \"/home/ubuntu/gdino/GroundingDINO\"\n",
    "IMAGE_PATH = f\"{GDINO_PATH}/.asset/cat_dog.jpeg\"\n",
    "TEXT_PROMPT = \"chair . person . dog .\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_source, image = load_image(IMAGE_PATH)\n",
    "\n",
    "boxes, logits, phrases = predict(\n",
    "    model=model,\n",
    "    image=image,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from opencv-python) (1.25.2)\n",
      "Downloading opencv_python-4.9.0.80-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.9.0.80\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Tuple\n",
    "import cv2\n",
    "from scipy.fftpack import sc_diff\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "from typing import Any, Generator\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import groundingdino.datasets.transforms as T\n",
    "\n",
    "\n",
    "def extract_frames(video_path, output_folder, frames_limit=100, skip=0):\n",
    "    \"\"\"\n",
    "        write each frame to a file\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Check if video opened successfully\n",
    "    if not video.isOpened():\n",
    "        print(\"Could not open video\")\n",
    "        return\n",
    "\n",
    "    frame_taken = 0\n",
    "    iteration = -1\n",
    "    success = True\n",
    "    files = []\n",
    "    while (frames_limit > 0 and frame_taken < frames_limit) or (frames_limit == 0 and success is True):\n",
    "        iteration += 1\n",
    "        \n",
    "        # Read the next frame from the video. If you read at the end of the video, success will be False\n",
    "        success, frame = video.read()\n",
    "        # print(frame_count)\n",
    "\n",
    "        # Break the loop if the video is finished\n",
    "        if not success:\n",
    "            break\n",
    "        if skip != 0 and iteration % skip != 0:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "\n",
    "        # Save the frame into the output folder\n",
    "        cv2.imwrite(f\"{output_folder}/frame{frame_taken}.jpg\", frame)\n",
    "        files.append(f\"{output_folder}/frame{frame_taken}.jpg\")\n",
    "\n",
    "        frame_taken +=1\n",
    "\n",
    "    # Release the video file\n",
    "    video.release()\n",
    "    return files\n",
    "\n",
    "def generate_frames(video_file: str, frames_limit=10) -> Generator[np.ndarray, None, None]:\n",
    "    \"\"\"\n",
    "        yield each frame as byte array\n",
    "    \"\"\"\n",
    "    video = cv2.VideoCapture(video_file)\n",
    "    frame_count = 0\n",
    "\n",
    "    while video.isOpened():\n",
    "        success, frame = video.read()\n",
    "\n",
    "        if not ((frames_limit > 0 and frame_count < frames_limit) or (frames_limit == 0 and success is True)):\n",
    "            break\n",
    "\n",
    "        yield frame\n",
    "        frame_count += 1\n",
    "\n",
    "    video.release()\n",
    "\n",
    "def plot_image(image: np.ndarray, size: int = 12) -> None:\n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(size, size))\n",
    "    plt.imshow(image[...,::-1])\n",
    "    plt.show()\n",
    "\n",
    "def zoom_at(img, zoom=1, angle=0, coord=None):\n",
    "    \n",
    "    cy, cx = [ i/2 for i in img.shape[:-1] ] if coord is None else coord[::-1]\n",
    "    \n",
    "    rot_mat = cv2.getRotationMatrix2D((cx,cy), angle, zoom)\n",
    "    result = cv2.warpAffine(img, rot_mat, img.shape[1::-1], flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def convert_ndarray(frame: np.ndarray[Any]) ->  torch.Tensor:\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_source = Image.fromarray(frame_rgb)\n",
    "\n",
    "    # image_source = Image.fromarray(arr).convert(\"RGB\")\n",
    "    # image_source = Image.open(image_path).convert(\"RGB\")\n",
    "    # image_source = Image.open(image_path).convert(\"RGB\")\n",
    "    # image = np.asarray(image_source)\n",
    "    image_transformed, _ = transform(image_source, None)\n",
    "    return image_transformed\n",
    "# Useage\n",
    "# frame_iterator = iter(generate_frames(video_file=SOURCE_VIDEO_PATH))\n",
    "# frame = next(frame_iterator)\n",
    "# plot_image(frame, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['output/mid/frame0.jpg',\n",
       " 'output/mid/frame1.jpg',\n",
       " 'output/mid/frame2.jpg',\n",
       " 'output/mid/frame3.jpg',\n",
       " 'output/mid/frame4.jpg',\n",
       " 'output/mid/frame5.jpg',\n",
       " 'output/mid/frame6.jpg',\n",
       " 'output/mid/frame7.jpg',\n",
       " 'output/mid/frame8.jpg',\n",
       " 'output/mid/frame9.jpg',\n",
       " 'output/mid/frame10.jpg',\n",
       " 'output/mid/frame11.jpg',\n",
       " 'output/mid/frame12.jpg',\n",
       " 'output/mid/frame13.jpg',\n",
       " 'output/mid/frame14.jpg',\n",
       " 'output/mid/frame15.jpg',\n",
       " 'output/mid/frame16.jpg',\n",
       " 'output/mid/frame17.jpg',\n",
       " 'output/mid/frame18.jpg',\n",
       " 'output/mid/frame19.jpg',\n",
       " 'output/mid/frame20.jpg',\n",
       " 'output/mid/frame21.jpg',\n",
       " 'output/mid/frame22.jpg',\n",
       " 'output/mid/frame23.jpg',\n",
       " 'output/mid/frame24.jpg',\n",
       " 'output/mid/frame25.jpg',\n",
       " 'output/mid/frame26.jpg',\n",
       " 'output/mid/frame27.jpg']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = extract_frames(\n",
    "    video_path=\"input/basketball.mp4\", \n",
    "    output_folder=\"output/mid\", \n",
    "    frames_limit=100, \n",
    "    skip=100\n",
    ")\n",
    "files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1520, 2704), 59.94005994005994)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "vid_cap = cv2.VideoCapture('input/basketball.mp4')\n",
    "fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "ret, frame = vid_cap.read()\n",
    "vid_cap.release()\n",
    "resolution_size = (int(frame.shape[0]), int(frame.shape[1]))\n",
    "resolution_size, fps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "# size = (int(img0.shape[1]/2), int(img0.shape[0]/2))\n",
    "resolution = (int(1024), int(1024))\n",
    "# Create a new video\n",
    "h264 = cv.VideoWriter_fourcc('h','2','6','4')\n",
    "mp4v = cv.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "new_video = cv2.VideoWriter(\"new_video1.mp4\", mp4v, fps, resolution)\n",
    "\n",
    "for file in files:\n",
    "    f_name = os.path.basename(file)\n",
    "    print(f_name)\n",
    "    image_source, image = load_image(file)\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=image,\n",
    "        caption=\"basketball\",\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "    if boxes.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    x = xyxy[0][0]\n",
    "    y = xyxy[0][1]\n",
    "\n",
    "    # cv2.imwrite(f\"output/predict/{f_name}\", annotated_frame)\n",
    "    # cv.imwrite('zoom_frame0.jpg', zoom_at(annotated_frame, 1.5, coord=(264.5, 275)) )\n",
    "    \n",
    "    # plot_image(annotated_frame, 8)\n",
    "    # plot_image(zoom_at(annotated_frame, 2, coord=(x, y)), 1280)\n",
    "    new_video.write(annotated_frame)\n",
    "    # print(boxes, logits, phrases)\n",
    "    print(\"--------------\")\n",
    "\n",
    "print(\"Releasing video\")\n",
    "new_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -y -c conda-forge ipywidgets # or pip install ipywidgets\n",
    "# !conda install -n base -c conda-forge jupyterlab_widgets\n",
    "\n",
    "# !conda install -n base -c conda-forge widgetsnbextension\n",
    "# !which python\n",
    "# !pip install ipywidgets widgetsnbextension pandas-profiling\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from tqdm.notebook import tqdm\n",
    "import groundingdino.datasets.transforms as T\n",
    "import PIL\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "def load_ndarr_image(numpy_image) -> Tuple[np.array, torch.Tensor]:\n",
    "    transform = T.Compose(\n",
    "        [\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )\n",
    "    # image_source = Image.open(image_path).convert(\"RGB\")\n",
    "    PIL_image = Image.fromarray(np.uint8(numpy_image)).convert('RGB')\n",
    "\n",
    "    image = np.asarray(PIL_image)\n",
    "    image_transformed, _ = transform(PIL_image, None)\n",
    "    return image, image_transformed\n",
    "\n",
    "frame_iterator = iter(generate_frames(video_file=\"input/basketball.mp4\", frames_limit=5))\n",
    "frames_data = []\n",
    "for frame in tqdm(frame_iterator, total=5):\n",
    "    # print(frame)\n",
    "    print(\"-------------------\")\n",
    "    image_source, image = load_ndarr_image(frame)\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=image,\n",
    "        caption=\"basketball\",\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "    frames_data.append((boxes, logits, phrases))\n",
    "\n",
    "    annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    h, w, _ = image_source.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "\n",
    "    # cv2.imwrite(f\"output/predict/{f_name}\", annotated_frame)\n",
    "    print(boxes, logits, phrases, xyxy)\n",
    "    plot_image(annotated_frame, 16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zooming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "\n",
    "img = cv.imread('output/mid/frame0.jpg')\n",
    "z_img = zoom_at(img, 1.5, coord=(264.5, 275))\n",
    "# print(z_img)\n",
    "    \n",
    "\n",
    "cv.imwrite('frame0.jpg', img )\n",
    "cv.imwrite('zoom_frame0.jpg', zoom_at(img, 1.5, coord=(264.5, 275)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install -y ffmpeg x264 libx264-dev\n",
    "# !pip install jupyterlab_widgets ipywidgets\n",
    "!pip install --upgrade ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n",
      "----------- 1668703592.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x5634504d/'MP4V' is not supported with codec id 12 and format 'mp4 / MP4 (MPEG-4 Part 14)'\n",
      "OpenCV: FFMPEG: fallback to use tag 0x7634706d/'mp4v'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cec54599dc49fe857a212ae145f43d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 0tensor([[0.1577, 0.5704, 0.0076, 0.0113],\n",
      "        [0.1913, 0.5462, 0.0064, 0.0083]]) tensor([0.5520, 0.3896]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 1tensor([[0.1577, 0.5730, 0.0071, 0.0115]]) tensor([0.4232]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 2tensor([[0.1570, 0.5752, 0.0079, 0.0124]]) tensor([0.4797]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 3tensor([[0.1577, 0.5777, 0.0061, 0.0113]]) tensor([0.4626]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 4tensor([[0.1567, 0.5808, 0.0075, 0.0124]]) tensor([0.5346]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 5tensor([[0.1559, 0.5835, 0.0093, 0.0118],\n",
      "        [0.2397, 0.5285, 0.0086, 0.0106]]) tensor([0.5573, 0.3558]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 6tensor([[0.1559, 0.5867, 0.0085, 0.0104]]) tensor([0.5060]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 7tensor([[0.1559, 0.5883, 0.0082, 0.0123],\n",
      "        [0.2358, 0.5292, 0.0073, 0.0112]]) tensor([0.4638, 0.3593]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 8tensor([[0.1550, 0.5916, 0.0089, 0.0115]]) tensor([0.5063]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 9tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 9tensor([[0.1551, 0.5953, 0.0089, 0.0106]]) tensor([0.4494]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 10tensor([[0.1552, 0.5963, 0.0089, 0.0114]]) tensor([0.4602]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 11tensor([[0.2298, 0.5315, 0.0071, 0.0088],\n",
      "        [0.1553, 0.5969, 0.0084, 0.0109]]) tensor([0.3597, 0.3984]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 12tensor([[0.1554, 0.5976, 0.0087, 0.0117]]) tensor([0.3910]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 13tensor([[0.1558, 0.5979, 0.0086, 0.0122]]) tensor([0.3935]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 14tensor([[0.1561, 0.5979, 0.0086, 0.0122]]) tensor([0.3612]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 15tensor([[0.1565, 0.5979, 0.0086, 0.0119]]) tensor([0.3938]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 16tensor([[0.1573, 0.5977, 0.0078, 0.0113]]) tensor([0.3830]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 17tensor([[0.1580, 0.5977, 0.0076, 0.0111]]) tensor([0.3746]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 18tensor([[0.1587, 0.5977, 0.0081, 0.0111]]) tensor([0.3584]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 19tensor([[0.1590, 0.5976, 0.0090, 0.0120]]) tensor([0.4662]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 20tensor([[0.1593, 0.5973, 0.0094, 0.0124]]) tensor([0.5283]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 21tensor([[0.1596, 0.5969, 0.0093, 0.0133]]) tensor([0.5786]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 22tensor([[0.1595, 0.5966, 0.0097, 0.0141]]) tensor([0.6654]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 23tensor([[0.1596, 0.5962, 0.0097, 0.0142]]) tensor([0.6939]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 24tensor([[0.1597, 0.5959, 0.0095, 0.0135]]) tensor([0.7108]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 25tensor([[0.1597, 0.5958, 0.0095, 0.0132]]) tensor([0.7067]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 26tensor([[0.1598, 0.5954, 0.0095, 0.0132]]) tensor([0.7110]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 27tensor([[0.1599, 0.5950, 0.0092, 0.0136]]) tensor([0.7201]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 28tensor([[0.1599, 0.5947, 0.0091, 0.0135]]) tensor([0.7151]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 29tensor([[0.1599, 0.5946, 0.0093, 0.0135]]) tensor([0.7113]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 30tensor([[0.1600, 0.5946, 0.0090, 0.0132]]) tensor([0.6840]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 31tensor([[0.1599, 0.5950, 0.0090, 0.0124]]) tensor([0.6684]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 32tensor([[0.1598, 0.5953, 0.0084, 0.0121]]) tensor([0.6669]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 33tensor([[0.1594, 0.5959, 0.0087, 0.0130]]) tensor([0.6734]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 34tensor([[0.1588, 0.5963, 0.0093, 0.0125]]) tensor([0.6535]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 35tensor([[0.1581, 0.5973, 0.0100, 0.0121]]) tensor([0.6510]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 36tensor([[0.1574, 0.5979, 0.0099, 0.0116]]) tensor([0.6104]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 37tensor([[0.1569, 0.5983, 0.0093, 0.0097]]) tensor([0.4644]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 38tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 38tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 38tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 38tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 38tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 38tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 38tensor([[0.1587, 0.5979, 0.0098, 0.0106]]) tensor([0.3769]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 39tensor([[0.1597, 0.5980, 0.0100, 0.0107]]) tensor([0.4158]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 40tensor([[0.1613, 0.5980, 0.0101, 0.0113]]) tensor([0.4474]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 41tensor([[0.1634, 0.5981, 0.0096, 0.0107]]) tensor([0.4709]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 42tensor([[0.1653, 0.5979, 0.0099, 0.0108]]) tensor([0.4205]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 43tensor([[0.1665, 0.5977, 0.0088, 0.0101]]) tensor([0.3853]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 44tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 44tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 44tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 44tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 44tensor([[0.1612, 0.5716, 0.0072, 0.0113]]) tensor([0.3721]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 45tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 45tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 45tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 45tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 45tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 45tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 45tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 45tensor([[0.2240, 0.5241, 0.0068, 0.0112]]) tensor([0.3804]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 46tensor([[0.2238, 0.5236, 0.0063, 0.0102]]) tensor([0.3548]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 47tensor([[0.2238, 0.5236, 0.0063, 0.0099]]) tensor([0.3554]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 48tensor([[0.2238, 0.5225, 0.0056, 0.0081]]) tensor([0.3717]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 49tensor([[0.2240, 0.5227, 0.0056, 0.0084]]) tensor([0.3646]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 50tensor([[0.2239, 0.5247, 0.0062, 0.0114]]) tensor([0.4168]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 51tensor([[0.2240, 0.5248, 0.0064, 0.0115]]) tensor([0.4238]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 52tensor([[0.2241, 0.5241, 0.0063, 0.0102],\n",
      "        [0.1922, 0.5711, 0.0047, 0.0095]]) tensor([0.4292, 0.3519]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 53tensor([[0.2245, 0.5234, 0.0062, 0.0086],\n",
      "        [0.2045, 0.6087, 0.0070, 0.0118]]) tensor([0.4037, 0.3515]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 54tensor([[0.2046, 0.6066, 0.0070, 0.0117],\n",
      "        [0.2252, 0.5229, 0.0061, 0.0079]]) tensor([0.5185, 0.3916]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 55tensor([[0.2052, 0.6048, 0.0065, 0.0108],\n",
      "        [0.2256, 0.5232, 0.0059, 0.0076]]) tensor([0.5060, 0.3859]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 56tensor([[0.2052, 0.6030, 0.0078, 0.0111]]) tensor([0.4690]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 57tensor([[0.2061, 0.6016, 0.0076, 0.0107]]) tensor([0.4396]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 58tensor([[0.2288, 0.5278, 0.0082, 0.0139]]) tensor([0.3557]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 59tensor([[0.2064, 0.5984, 0.0078, 0.0108]]) tensor([0.3536]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 60tensor([[0.2068, 0.5958, 0.0093, 0.0133],\n",
      "        [0.2293, 0.5287, 0.0074, 0.0132]]) tensor([0.4464, 0.3588]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 61tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 61tensor([[0.2299, 0.5288, 0.0071, 0.0138]]) tensor([0.3900]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 62tensor([[0.2301, 0.5287, 0.0068, 0.0135]]) tensor([0.4078]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 63tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 63tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 63tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 63tensor([[0.2117, 0.5871, 0.0079, 0.0115]]) tensor([0.4350]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 64tensor([[0.2125, 0.5868, 0.0097, 0.0120]]) tensor([0.3929]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 65tensor([[0.2137, 0.5858, 0.0094, 0.0104]]) tensor([0.4198]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 66tensor([[0.2150, 0.5855, 0.0092, 0.0118]]) tensor([0.4337]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 67tensor([[0.2170, 0.5853, 0.0078, 0.0121]]) tensor([0.4817]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 68tensor([[0.2185, 0.5847, 0.0076, 0.0124],\n",
      "        [0.2471, 0.5408, 0.0052, 0.0095]]) tensor([0.3997, 0.3799]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 69tensor([[0.2202, 0.5838, 0.0082, 0.0130]]) tensor([0.5613]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 70tensor([[0.2223, 0.5819, 0.0081, 0.0116],\n",
      "        [0.2470, 0.5407, 0.0053, 0.0087]]) tensor([0.5205, 0.3618]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 71tensor([[0.2242, 0.5808, 0.0082, 0.0117],\n",
      "        [0.2470, 0.5407, 0.0053, 0.0087]]) tensor([0.4666, 0.3803]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 72tensor([[0.2258, 0.5796, 0.0084, 0.0110],\n",
      "        [0.2471, 0.5407, 0.0052, 0.0084]]) tensor([0.3771, 0.3507]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 73tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 73tensor([[0.2470, 0.5408, 0.0052, 0.0086]]) tensor([0.3763]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 74tensor([[0.2469, 0.5410, 0.0053, 0.0086]]) tensor([0.3880]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 75tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 75tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 75tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 75tensor([[0.1943, 0.5708, 0.0068, 0.0109]]) tensor([0.3648]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 76tensor([[0.2414, 0.5689, 0.0069, 0.0102],\n",
      "        [0.1945, 0.5709, 0.0068, 0.0108]]) tensor([0.4128, 0.3995]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 77tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 77tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 77tensor([[0.2450, 0.5633, 0.0083, 0.0114]]) tensor([0.4866]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 78tensor([[0.2460, 0.5617, 0.0081, 0.0116]]) tensor([0.5307]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 79tensor([[0.2466, 0.5605, 0.0073, 0.0119]]) tensor([0.4439]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 80tensor([[0.2470, 0.5578, 0.0070, 0.0126]]) tensor([0.4664]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 81tensor([[0.2467, 0.5556, 0.0073, 0.0135]]) tensor([0.4913]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 82tensor([[0.2467, 0.5544, 0.0071, 0.0134]]) tensor([0.3991]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 83tensor([[0.2470, 0.5530, 0.0063, 0.0128]]) tensor([0.3735]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 84tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 84tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 84tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 84tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 84tensor([], size=(0, 4)) tensor([]) []\n",
      "Count: 84tensor([[0.2533, 0.5422, 0.0122, 0.0266],\n",
      "        [0.2377, 0.5377, 0.0078, 0.0167]]) tensor([0.4193, 0.3719]) ['basketball', 'basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 85tensor([[0.2579, 0.5321, 0.0088, 0.0121]]) tensor([0.4535]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Count: 86tensor([[0.2622, 0.5281, 0.0071, 0.0112]]) tensor([0.5805]) ['basketball']\n",
      "------------------- <class 'numpy.ndarray'>\n",
      "Releasing video\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "import torch\n",
    "from torchvision.ops import box_convert\n",
    "\n",
    "h264 = cv2.VideoWriter_fourcc('h','2','6','4')\n",
    "mp4v = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "mp4v_2 = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "vid_name = \"output/new_video1.mp4\"\n",
    "\n",
    "\n",
    "# vid_cap = cv.VideoCapture(\"input/basketball.mp4\")\n",
    "\n",
    "# fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "# ret, frame = vid_cap.read()\n",
    "# vid_cap.release()\n",
    "# resolution_size = (int(frame.shape[0]), int(frame.shape[1]))\n",
    "\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"weights/groundingdino_swint_ogc.pth\")\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "cap = cv2.VideoCapture(\"input/basketball.mp4\")\n",
    "\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# cap.open(\"input/basketball.mp4\")\n",
    "exist_fourcc = cap.get(cv2.CAP_PROP_FOURCC)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "\n",
    "cap.release()\n",
    "print(\"-----------\", exist_fourcc)\n",
    "# return\n",
    "try:\n",
    "    os.remove(vid_name)\n",
    "except:\n",
    "    pass\n",
    "new_video = cv2.VideoWriter(vid_name, fourcc, fps, (width, height))\n",
    "\n",
    "\n",
    "if not new_video.isOpened():\n",
    "    print(\"Error: Could not open output video.\")\n",
    "    exit()\n",
    "\n",
    "frame_iterator = iter(generate_frames(video_file=\"input/basketball.mp4\", frames_limit=120))\n",
    "frames_data = []\n",
    "counter = 0\n",
    "for frame in tqdm(frame_iterator, total=5):\n",
    "    # print(frame)\n",
    "    print(f'\\rCount: {counter}', end='', flush=True)\n",
    "    \n",
    "    transformed_array = convert_ndarray(frame)\n",
    "\n",
    "\n",
    "    boxes, logits, phrases = predict(\n",
    "        model=model,\n",
    "        image=transformed_array,\n",
    "        caption=\"basketball\",\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD\n",
    "    )\n",
    "    print(boxes, logits, phrases)\n",
    "    if boxes.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    annotated_frame = annotate(image_source=frame, boxes=boxes, logits=logits, phrases=phrases)\n",
    "    h, w, _ = frame.shape\n",
    "    boxes = boxes * torch.Tensor([w, h, w, h])\n",
    "    xyxy = box_convert(boxes=boxes, in_fmt=\"cxcywh\", out_fmt=\"xyxy\").numpy()\n",
    "    x = xyxy[0][0]\n",
    "    y = xyxy[0][1]\n",
    "    zoom_frame = zoom_at(annotated_frame, 2, coord=(x, y))\n",
    "\n",
    "    print(\"-------------------\", type(frame))\n",
    "    new_video.write(zoom_frame)\n",
    "    # new_video.write(frame)\n",
    "    counter += 1\n",
    "\n",
    "print(\"Releasing video\")\n",
    "new_video.release()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
